{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1EsRHo7Rx6rphVYoMVjr9G3GtOOoF6Ved",
      "authorship_tag": "ABX9TyNDLuwjuwI6nApanCxHGVnY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mefiuuu1/OOPLabs2024/blob/main/proiect_bac.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        },
        "id": "Qhzu1iYJc77E",
        "outputId": "05bb79a0-e353-4468-ce63-768c50e2d2f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Am încărcat 7 texte.\n",
            "{'input_ids': tensor([[26661,   352,  1149,  ...,   607,  4816,    12],\n",
            "        [ 5097,  8828, 41708,  ...,     0,     0,     0],\n",
            "        [ 5097,  8828, 31467,  ...,     0,     0,     0],\n",
            "        ...,\n",
            "        [   48, 21916, 45114,  ...,     0,     0,     0],\n",
            "        [   48, 21916, 45114,  ...,     0,     0,     0],\n",
            "        [   48, 21916, 45114,  ...,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        ...,\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0]])}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2000' max='2000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2000/2000 31:59, Epoch 500/500]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.205400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.003200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.002200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.001900</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments\n",
        "from datasets import Dataset\n",
        "import os\n",
        "\n",
        "def incarca_date():\n",
        "    dataset = []\n",
        "    # Textul integral\n",
        "    with open(\"/content/drive/MyDrive/proiect_bac/ion/text_integral.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "        dataset.append(f.read())\n",
        "    # Comentarii\n",
        "    for root, dirs, files in os.walk(\"/content/drive/MyDrive/proiect_bac/ion/comentarii\"):\n",
        "        for file in files:\n",
        "            with open(os.path.join(root, file), \"r\", encoding=\"utf-8\") as f:\n",
        "                dataset.append(f.read())\n",
        "    print(f\"Am încărcat {len(dataset)} texte.\")  # Verifică câte texte au fost încărcate\n",
        "    return dataset\n",
        "\n",
        "# 1. Setează pad_token\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"readerbench/RoGPT2-medium\")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# 2. Încarcă modelul\n",
        "model = GPT2LMHeadModel.from_pretrained(\"readerbench/RoGPT2-medium\")\n",
        "\n",
        "# 3. Tokenizare cu padding și truncation\n",
        "texts = incarca_date()\n",
        "inputs = tokenizer(\n",
        "    texts,\n",
        "    return_tensors=\"pt\",\n",
        "    padding=\"max_length\",\n",
        "    truncation=True,\n",
        "    max_length=512\n",
        ")\n",
        "print(inputs)  # Verifică ce returnează tokenizer-ul\n",
        "\n",
        "# 4. Antrenare\n",
        "\n",
        "train_dataset = Dataset.from_dict({\n",
        "    \"input_ids\": inputs[\"input_ids\"],\n",
        "    \"attention_mask\": inputs[\"attention_mask\"],\n",
        "    \"labels\": inputs[\"input_ids\"],  # For language modeling, labels are the same as input_ids\n",
        "})\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"model-ion\",\n",
        "    per_device_train_batch_size=2,\n",
        "    num_train_epochs=500,\n",
        "    learning_rate=3e-5,\n",
        "    weight_decay=0.01,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        ")\n",
        "trainer.train()\n",
        "\n",
        "# Save the tokenizer and model configuration after training\n",
        "tokenizer.save_pretrained(\"model-ion\")  # Save tokenizer to \"model-ion\" directory\n",
        "model.save_pretrained(\"model-ion\")  # Save model to \"model-ion\" directory"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "ZGtHoc3eEYq2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer, AutoConfig\n",
        "\n",
        "# Încarcă tokenizer-ul mai întâi\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"model-ion\", trust_remote_code=True)  # Add trust_remote_code=True\n",
        "\n",
        "# Încarcă configurația modelului\n",
        "config = AutoConfig.from_pretrained(\"model-ion\", trust_remote_code=True)\n",
        "\n",
        "# Apoi, încarcă modelul folosind configurația\n",
        "model = AutoModelForCausalLM.from_pretrained(\"model-ion\", trust_remote_code=True, config=config)  # Add trust_remote_code=True\n",
        "\n",
        "# Salvare explicită a configurației și tokenizer-ului în timpul antrenării\n",
        "# (Adaugă aceste linii în scriptul de antrenare după trainer.train())\n",
        "tokenizer.save_pretrained(\"model-ion\")\n",
        "model.config.save_pretrained(\"model-ion\")\n",
        "\n",
        "\n",
        "# Creează generatorul de text\n",
        "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "# Generează un comentariu\n",
        "prompt = \"\"\"\n",
        "Redactează un eseu de minimum 400 de cuvinte, în care să prezinți particularități ale romanului „Ion” de Liviu Rebreanu.\n",
        "\n",
        "În elaborarea eseului, vei avea în vedere următoarele repere:\n",
        "\n",
        "– Evidențierea a două trăsături distinctive care fac posibilă încadrarea romanului „Ion” într-o anumită perioadă istorică, într-un curent literar sau într-o orientare tematică specifică (de exemplu, realismul, contextul rural sau lupta pentru pământ).\n",
        "– Comentarea a două episoade sau secvențe relevante din roman, care evidențiază temele principale și complexitatea personajelor, cum ar fi conflictul dintre destin și voința individuală sau relațiile tensionate dintre personaje.\n",
        "– Analiza a două elemente de structură, compoziție și/sau de limbaj, semnificative pentru romanul „Ion”. Acestea pot include, de exemplu, analiza modului în care acțiunea este organizată, conflictul central, modul de redare a relațiilor temporale și spațiale, tehnicile narative folosite (cum ar fi perspectiva narativă sau registrele stilistice) și impactul acestora asupra înțelegerii mesajului autorului.\n",
        "\n",
        "Răspuns:\n",
        "\"\"\"\n",
        "comentariu = generator(\n",
        "    prompt,\n",
        "    max_length=700,\n",
        "    num_return_sequences=1,\n",
        "    temperature=0.7,\n",
        "    top_k=100,\n",
        "    repetition_penalty=1.2)[0][\"generated_text\"]\n",
        "print(comentariu)"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t-TDFbNjdqmb",
        "outputId": "08f3a561-29a3-4be5-bbf5-88fa29ff7853"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Redactează un eseu de minimum 400 de cuvinte, în care să prezinți particularități ale romanului „Ion” de Liviu Rebreanu.\n",
            "\n",
            "În elaborarea eseului, vei avea în vedere următoarele repere:\n",
            "\n",
            "– Evidențierea a două trăsături distinctive care fac posibilă încadrarea romanului „Ion” într-o anumită perioadă istorică, într-un curent literar sau într-o orientare tematică specifică (de exemplu, realismul, contextul rural sau lupta pentru pământ).\n",
            "– Comentarea a două episoade sau secvențe relevante din roman, care evidențiază temele principale și complexitatea personajelor, cum ar fi conflictul dintre destin și voința individuală sau relațiile tensionate dintre personaje.\n",
            "– Analiza a două elemente de structură, compoziție și/sau de limbaj, semnificative pentru romanul „Ion”. Acestea pot include, de exemplu, analiza modului în care acțiunea este organizată, conflictul central, modul de redare a relațiilor temporale și spațiale, tehnicile narative folosite (cum ar fi perspectiva narativă sau registrele stilistice) și impactul acestora asupra înțelegerii mesajului autorului.\n",
            "\n",
            "Răspuns:\n",
            "Rebreanu critică obiectivismul excesiv al operei prin sublinierea aspectelor pozitive precum onestitatea umană și sacrificiul uman, dar și criticarea valorilor umane inferioare precum interesul îngust și dorința de putere.\n",
            "\n",
            "Exemple din text preluate din opera omonimă:\n",
            "1. \"Să mor de n-am să fiu stăpân pe un petic de pământ!\"  \n",
            "2. \"Nu sunt om până nu am pământul meu.\"  \n",
            "3. \"Ana nu zise nimic. Se uită lung la Ion și începu să plângă încet.\"  \n",
            "\n",
            "Concluzie:\n",
            "Rebreanu critică obiectivismul excesiv al operei prin sublinierea aspectelor pozitive precum onestitatea umană și sacrificiul uman, dar și criticarea valorilor umane inferioare precum interesul îngust și dorința de putere.\n"
          ]
        }
      ]
    }
  ]
}